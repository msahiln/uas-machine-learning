# -*- coding: utf-8 -*-
"""UAS sistem rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mSnCyxjSFlfrJdMg_5koYwJtVv4j9Q1d

## Persiapan & Download Data
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

import pandas as pd
import numpy as np

# Download dataset
!kaggle datasets download -d hernan4444/anime-recommendation-database-2020
!unzip anime-recommendation-database-2020.zip

"""##Data Understanding"""

import pandas as pd

# Membaca semua file utama yang ada di dataset
anime = pd.read_csv('anime.csv')
anime_with_synopsis = pd.read_csv('anime_with_synopsis.csv')
animelist = pd.read_csv('animelist.csv')
rating_complete = pd.read_csv('rating_complete.csv')
watching_status = pd.read_csv('watching_status.csv')

# Print jumlah data unik untuk masing-masing file
print('Jumlah data judul anime (anime.csv): ', len(anime.MAL_ID.unique()))
print('Jumlah data anime dengan sinopsis: ', len(anime_with_synopsis.MAL_ID.unique()))
print('Jumlah data list anime user (animelist): ', len(animelist.user_id.unique()))
print('Jumlah data rating lengkap: ', len(rating_complete.user_id.unique()))
print('Jumlah kategori status menonton: ', len(watching_status.status.unique()))

"""## Univariate EDA"""

# Cek info dari dataframe anime
print("--- Info Data Anime ---")
anime.info()

# Cek jumlah genre unik dan daftar genrenya
print('\nBanyak tipe genre: ', len(anime.Genres.unique()))
print('Tipe genre: ', anime.Genres.unique())

# Melihat 5 data teratas dari rating_complete
print("\n--- 5 Data Teratas Rating ---")
print(rating_complete.head())

# Melihat statistik deskriptif dari rating
print('\n--- Statistik Rating ---')
print(rating_complete.rating.describe())

"""## Data Preprocessing"""

import numpy as np

# 1. Mengambil sampel 50.000 data rating agar proses lancar
rating_sample = rating_complete.sample(n=50000, random_state=42)

# 2. Menggabungkan data rating dengan info nama & genre dari file anime
all_anime_name = pd.merge(rating_sample, anime[['MAL_ID', 'Name', 'Genres']],
                          left_on='anime_id', right_on='MAL_ID', how='left')

# Tampilkan 5 data teratas hasil gabungan
all_anime_name.head()

"""## Data Preparation"""

# Cek missing value
print("Jumlah missing value:")
print(all_anime_name.isnull().sum())

# Jika ada yang kosong hapus
all_anime_clean = all_anime_name.dropna()

# Mengurutkan data berdasarkan ID
fix_anime = all_anime_clean.sort_values('MAL_ID', ascending=True)

# Menghapus data duplikat pada kolom MAL_ID
preparation = fix_anime.drop_duplicates('MAL_ID')
preparation

# Mengonversi data series menjadi list
anime_id = preparation['MAL_ID'].tolist()
anime_name = preparation['Name'].tolist()
anime_genre = preparation['Genres'].tolist()

print('Jumlah ID Anime:', len(anime_id))
print('Jumlah Nama Anime:', len(anime_name))
print('Jumlah Genre Anime:', len(anime_genre))

# Membuat dataframe baru
anime_new = pd.DataFrame({
    'id': anime_id,
    'anime_name': anime_name,
    'genre': anime_genre
})
anime_new.head()

"""## Model Development"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data genre
tf.fit(anime_new['genre'])

# Menampilkan nama-nama fitur genre yang ditemukan
print(tf.get_feature_names_out())

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(anime_new['genre'])

# Melihat ukuran matrix tf-idf
tfidf_matrix.shape

# Membuat dataframe untuk melihat matriks tf-idf
# Kolom diisi dengan jenis genre, baris diisi dengan nama anime
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=anime_new.anime_name
).sample(22, axis=1).sample(10, axis=0)

"""## Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_new['anime_name'], columns=anime_new['anime_name'])
print('Shape:', cosine_sim_df.shape)

# Melihat matriks kesamaan
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""##Mendapatkan Rekomendasi"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=anime_new[['anime_name', 'genre']], k=5):
    """
    Rekomendasi Anime berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama_anime : tipe data string (str)
                Nama Anime (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Dataframe similarity, simetrik, dengan anime sebagai indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua definisi dan fitur lainnya digunakan untuk mendefinisikan kemiripan
    k : tipe data int
        Jumlah rekomendasi yang diberikan
    ---
    """

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    closest = similarity_data.loc[nama_anime].to_numpy().argpartition(
        range(-1, -k, -1)
    )

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest_indices = closest[-1:-(k+2):-1]

    # Drop nama_anime agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest_indices = [i for i in closest_indices if similarity_data.index[i] != nama_anime]

    # Return data frame dari items
    return items.iloc[closest_indices[:k]]

# Cek dulu genre anime yang mau kita jadikan acuan (misal: Naruto)
anime_new[anime_new.anime_name.eq('Naruto')]

# Memanggil fungsi rekomendasi
# Ganti 'Naruto' dengan judul lain yang ada di dataset lo kalau mau coba yang lain
anime_recommendations('Naruto')

"""## Model Development - Collaborative Filtering"""

# Import library yang dibutuhkan
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

# Kita pakai data 'rating_sample' yang sudah kita buat tadi (biar tidak berat)
df = rating_sample

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Melakukan hal yang sama untuk anime_id
anime_ids = df['anime_id'].unique().tolist()
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

# Memetakan user_id dan anime_id ke dataframe yang berkaitan
df['user'] = df['user_id'].map(user_to_user_encoded)
df['anime'] = df['anime_id'].map(anime_to_anime_encoded)

# Mendapatkan jumlah user dan anime
num_users = len(user_to_user_encoded)
num_anime = len(anime_to_anime_encoded)

# Mengubah rating menjadi float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum dan maksimum rating
min_rating = min(df['rating'])
max_rating = max(df['rating'])

print(f'Number of User: {num_users}, Number of Anime: {num_anime}, Min Rating: {min_rating}, Max Rating: {max_rating}')

# Mengacak dataset
df = df.sample(frac=1, random_state=42)

# Membuat variabel x untuk input dan y untuk target
x = df[['user', 'anime']].values
y = (df['rating'] - min_rating) / (max_rating - min_rating) # Normalisasi rating

# Membagi data menjadi 80% train dan 20% test
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embedding anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding anime bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai proses training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 20,
    validation_data = (x_val, y_val)
)

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Menyiapkan variabel untuk testing
anime_df = anime_new
df = pd.read_csv('rating_complete.csv').sample(n=50000, random_state=42) # samakan dengan sample awal

# Mengambil sample user secara acak
user_id = df.user_id.sample(1).iloc[0]
anime_visited_by_user = df[df.user_id == user_id]

# Membuat daftar anime yang belum pernah ditonton oleh user
anime_not_visited = anime_df[~anime_df['id'].isin(anime_visited_by_user.anime_id.values)]['id']
anime_not_visited = list(
    set(anime_not_visited)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_visited = [[anime_to_anime_encoded.get(x)] for x in anime_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_visited), anime_not_visited)
)

# Melakukan prediksi skor rating untuk anime yang belum ditonton
ratings = model.predict(user_anime_array).flatten()

# Mengambil 10 rekomendasi tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.anime_name, ':', row.genre)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_df[anime_df['id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.anime_name, ':', row.genre)